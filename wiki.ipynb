{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "c3cb0864-4a2f-4022-9821-c0f3d159da71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from typing import Dict, Optional, Tuple, List\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForMultipleChoice\n",
    "from torch.utils.data import Dataset, random_split\n",
    "from datasets import Dataset as _Dataset, DatasetDict\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_score, f1_score, recall_score, classification_report\n",
    "import blingfire as bf\n",
    "from __future__ import annotations\n",
    "\n",
    "import time\n",
    "import math\n",
    "import matplotlib\n",
    "matplotlib.rcParams.update({'figure.figsize': (16, 12), 'font.size': 14})\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import faiss\n",
    "from faiss import write_index, read_index\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Union\n",
    "from collections.abc import Iterable\n",
    "import string\n",
    "\n",
    "import gc\n",
    "\n",
    "from src.data_loader import *\n",
    "from src.qa_dataset import *\n",
    "from src.train import *\n",
    "from src.classifiers import *\n",
    "from src.graph import *\n",
    "\n",
    "os.environ['WANDB_DISABLED'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fcb96c-dcf8-4f47-9e69-1507a2003677",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "851ed269-a86c-40b6-968d-75b1bbcb9d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_documents(documents: Iterable[str],\n",
    "                      document_ids: Iterable,\n",
    "                      split_sentences: bool = True,\n",
    "                      filter_len: int = 3,\n",
    "                      disable_progress_bar: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Main helper function to process documents from the EMR.\n",
    "\n",
    "    :param documents: Iterable containing documents which are strings\n",
    "    :param document_ids: Iterable containing document unique identifiers\n",
    "    :param document_type: String denoting the document type to be processed\n",
    "    :param document_sections: List of sections for a given document type to process\n",
    "    :param split_sentences: Flag to determine whether to further split sections into sentences\n",
    "    :param filter_len: Minimum character length of a sentence (otherwise filter out)\n",
    "    :param disable_progress_bar: Flag to disable tqdm progress bar\n",
    "    :return: Pandas DataFrame containing the columns `document_id`, `text`, `section`, `offset`\n",
    "    \"\"\"\n",
    "    \n",
    "    df = sectionize_documents(documents, document_ids, disable_progress_bar)\n",
    "\n",
    "    if split_sentences:\n",
    "        df = sentencize(df.text.values, \n",
    "                        df.document_id.values,\n",
    "                        df.offset.values, \n",
    "                        filter_len, \n",
    "                        disable_progress_bar)\n",
    "    return df\n",
    "\n",
    "\n",
    "def sectionize_documents(documents: Iterable[str],\n",
    "                         document_ids: Iterable,\n",
    "                         disable_progress_bar: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Obtains the sections of the imaging reports and returns only the \n",
    "    selected sections (defaults to FINDINGS, IMPRESSION, and ADDENDUM).\n",
    "\n",
    "    :param documents: Iterable containing documents which are strings\n",
    "    :param document_ids: Iterable containing document unique identifiers\n",
    "    :param disable_progress_bar: Flag to disable tqdm progress bar\n",
    "    :return: Pandas DataFrame containing the columns `document_id`, `text`, `offset`\n",
    "    \"\"\"\n",
    "    processed_documents = []\n",
    "    for document_id, document in tqdm(zip(document_ids, documents), total=len(documents), disable=disable_progress_bar):\n",
    "        row = {}\n",
    "        text, start, end = (document, 0, len(document))\n",
    "        row['document_id'] = document_id\n",
    "        row['text'] = text\n",
    "        row['offset'] = (start, end)\n",
    "\n",
    "        processed_documents.append(row)\n",
    "\n",
    "    _df = pd.DataFrame(processed_documents)\n",
    "    if _df.shape[0] > 0:\n",
    "        return _df.sort_values(['document_id', 'offset']).reset_index(drop=True)\n",
    "    else:\n",
    "        return _df\n",
    "\n",
    "\n",
    "def sentencize(documents: Iterable[str],\n",
    "               document_ids: Iterable,\n",
    "               offsets: Iterable[tuple[int, int]],\n",
    "               filter_len: int = 3,\n",
    "               disable_progress_bar: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Split a document into sentences. Can be used with `sectionize_documents`\n",
    "    to further split documents into more manageable pieces. Takes in offsets\n",
    "    to ensure that after splitting, the sentences can be matched to the\n",
    "    location in the original documents.\n",
    "\n",
    "    :param documents: Iterable containing documents which are strings\n",
    "    :param document_ids: Iterable containing document unique identifiers\n",
    "    :param offsets: Iterable tuple of the start and end indices\n",
    "    :param filter_len: Minimum character length of a sentence (otherwise filter out)\n",
    "    :return: Pandas DataFrame containing the columns `document_id`, `text`, `section`, `offset`\n",
    "    \"\"\"\n",
    "\n",
    "    document_sentences = []\n",
    "    for document, document_id, offset in tqdm(zip(documents, document_ids, offsets), total=len(documents), disable=disable_progress_bar):\n",
    "        try:\n",
    "            _, sentence_offsets = bf.text_to_sentences_and_offsets(document)\n",
    "            for o in sentence_offsets:\n",
    "                if o[1]-o[0] > filter_len:\n",
    "                    sentence = document[o[0]:o[1]]\n",
    "                    abs_offsets = (o[0]+offset[0], o[1]+offset[0])\n",
    "                    row = {}\n",
    "                    row['document_id'] = document_id\n",
    "                    row['text'] = sentence\n",
    "                    row['offset'] = abs_offsets\n",
    "                    document_sentences.append(row)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "    return pd.DataFrame(document_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54a2fdd-bb59-4445-890b-819e5ea58a3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2889656f-cd86-431a-8ece-9593e6c1bec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Main data\n",
    "train_df = pd.read_csv('./data/train.csv')\n",
    "dev_df = pd.read_csv('./data/dev.csv')\n",
    "test_df = pd.read_csv('./data/test.csv')\n",
    "\n",
    "full_df = pd.concat([train_df, dev_df, test_df], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "06663268-2f23-42ca-81fb-7ca6ffab54f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "351d4a3d-fe8f-4f80-9bd4-badf3a33287b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model\n",
    "SIM_MODEL = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "MAX_LENGTH = 384\n",
    "\n",
    "\n",
    "model = SentenceTransformer(SIM_MODEL, device='cuda')\n",
    "model.max_seq_length = MAX_LENGTH\n",
    "model = model.half()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e1eaf389-020f-47b3-8d01-cda8d425069a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "fba2cefb-b48a-45af-a7d2-4c6a721c28cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|███████████████████████████████████████████████████████████████████████| 284/284 [00:02<00:00, 137.64it/s]\n"
     ]
    }
   ],
   "source": [
    "prompt_embeddings = model.encode(full_df.question.unique(), batch_size=BATCH_SIZE, device=device, show_progress_bar=True, convert_to_tensor=True, normalize_embeddings=True)\n",
    "prompt_embeddings = prompt_embeddings.detach().cpu().numpy()\n",
    "_ = gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15432b0-3685-4d69-8e9b-139f741691fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "268845fe-867c-49cd-8da0-67a70d919ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load wiki index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "15230561-8645-47a4-965c-c66122fce955",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_index = read_index(\"./wiki/wikipedia_202307.index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f53f1ecf-974c-4fea-8a77-a89fdf544907",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wiki = pd.read_parquet(\"./wiki/wiki_2023_index.parquet\", columns=['id', 'file'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06fc8fe-fbc2-419f-944c-441ee268d9d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "65cd5030-69af-43d8-b772-1f7f44a511f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Serch in index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "68355782-2ed5-427e-9a78-dc2b23d5d207",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_score, search_index = wiki_index.search(prompt_embeddings, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "7f57ba54-9b94-4c90-94e1-f0386f1d1a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 4535/4535 [00:01<00:00, 2384.07it/s]\n"
     ]
    }
   ],
   "source": [
    "wikipedia_file_data = []\n",
    "\n",
    "for i, (scr, idx) in tqdm(enumerate(zip(search_score, search_index)), total=len(search_score)):\n",
    "    scr_idx = idx\n",
    "    _df = df_wiki.loc[scr_idx].copy()\n",
    "    _df['prompt_id'] = i\n",
    "    wikipedia_file_data.append(_df)\n",
    "wikipedia_file_data = pd.concat(wikipedia_file_data).reset_index(drop=True)\n",
    "wikipedia_file_data = wikipedia_file_data[['id', 'prompt_id', 'file']].drop_duplicates().sort_values(['file', 'id']).reset_index(drop=True)\n",
    "\n",
    "## Save memory - delete df since it is no longer necessary\n",
    "del df_wiki\n",
    "_ = gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "99ff7343-153c-44ca-9dce-381f51713d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 28/28 [01:46<00:00,  3.79s/it]\n"
     ]
    }
   ],
   "source": [
    "## Get the full text data\n",
    "wiki_text_data = []\n",
    "WIKI_PATH = './wiki'\n",
    "\n",
    "for file in tqdm(wikipedia_file_data.file.unique(), total=len(wikipedia_file_data.file.unique())):\n",
    "    _id = [str(i) for i in wikipedia_file_data[wikipedia_file_data['file']==file]['id'].tolist()]\n",
    "    _df = pd.read_parquet(f\"{WIKI_PATH}/{file}\", columns=['id', 'text'])\n",
    "\n",
    "    _df_temp = _df[_df['id'].isin(_id)].copy()\n",
    "    del _df\n",
    "    _ = gc.collect()\n",
    "    #libc.malloc_trim(0)\n",
    "    wiki_text_data.append(_df_temp)\n",
    "wiki_text_data = pd.concat(wiki_text_data).drop_duplicates().reset_index(drop=True)\n",
    "_ = gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "f5025397-7e4c-46f1-9c7b-fdca8bee3368",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 9343/9343 [00:00<00:00, 388493.93it/s]\n",
      "  8%|█████▉                                                                         | 709/9343 [00:02<00:28, 301.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 != 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|█████████████████▋                                                            | 2120/9343 [00:08<00:23, 304.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 != 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|██████████████████████████████▍                                               | 3651/9343 [00:13<00:16, 340.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 != 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████████████████████████████████████▍                                         | 4366/9343 [00:16<00:14, 338.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 != 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████████████████████████████████████████████████████▊                      | 6686/9343 [00:22<00:07, 367.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 != 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|█████████████████████████████████████████████████████████▍                    | 6884/9343 [00:23<00:05, 417.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 != 2\n",
      "1 != 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|██████████████████████████████████████████████████████████████▍               | 7474/9343 [00:25<00:05, 357.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 != 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|██████████████████████████████████████████████████████████████████▎           | 7950/9343 [00:26<00:03, 351.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 != 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|███████████████████████████████████████████████████████████████████████████▊  | 9086/9343 [00:29<00:00, 280.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 != 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 9343/9343 [00:30<00:00, 305.40it/s]\n"
     ]
    }
   ],
   "source": [
    "processed_wiki_text_data = process_documents(wiki_text_data.text.values, wiki_text_data.id.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "61e72471-9c9f-4604-8b6a-a73597282e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|███████████████████████████████████████████████████████████████████| 39629/39629 [05:38<00:00, 117.01it/s]\n"
     ]
    }
   ],
   "source": [
    "wiki_data_embeddings = model.encode(processed_wiki_text_data.text,\n",
    "                                    batch_size=BATCH_SIZE,\n",
    "                                    device=device,\n",
    "                                    show_progress_bar=True,\n",
    "                                    convert_to_tensor=True,\n",
    "                                    normalize_embeddings=True)#.half()\n",
    "wiki_data_embeddings = wiki_data_embeddings.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "b4e41d69-354d-41c2-aa4d-1c00806bdedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "1a106466-a227-4cc6-bcbc-6d738ec4d06f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_207306/4203645439.py:4: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  cmb_df = full_df.groupby(\"question\").apply(combine_answers).reset_index()\n"
     ]
    }
   ],
   "source": [
    "def combine_answers(df):\n",
    "    return \" \".join(df.answerEntity.tolist())\n",
    "\n",
    "cmb_df = full_df.groupby(\"question\").apply(combine_answers).reset_index()\n",
    "cmb_df.columns = ['question', 'answer_all']\n",
    "cmb_df['prompt_answer_stem'] = cmb_df['question'] + \" \" + cmb_df['answer_all']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "16a53d64-54a6-4562-95e7-15f1b3830ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████████████████████████████████████████████████████████████████████| 71/71 [00:01<00:00, 62.71it/s]\n"
     ]
    }
   ],
   "source": [
    "question_embeddings = model.encode(cmb_df.prompt_answer_stem.values, batch_size=BATCH_SIZE, device=device, show_progress_bar=True, convert_to_tensor=True, normalize_embeddings=True)\n",
    "question_embeddings = question_embeddings.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "767cdb19-5e43-4ca1-8efa-12327ab1e0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "3cb4c36a-493e-4adc-97c4-55ff2ec4890b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 4535/4535 [02:14<00:00, 33.74it/s]\n"
     ]
    }
   ],
   "source": [
    "NUM_SENTENCES_INCLUDE = 4\n",
    "\n",
    "## List containing just Context\n",
    "contexts = []\n",
    "\n",
    "for r in tqdm(cmb_df.itertuples(), total=len(cmb_df)):\n",
    "\n",
    "    prompt_id = r.Index\n",
    "\n",
    "    prompt_indices = processed_wiki_text_data[processed_wiki_text_data['document_id'].isin(wikipedia_file_data[wikipedia_file_data['prompt_id']==prompt_id]['id'].values)].index.values\n",
    "\n",
    "    if prompt_indices.shape[0] > 0:\n",
    "        prompt_index = faiss.index_factory(wiki_data_embeddings.shape[1], \"Flat\")\n",
    "        prompt_index.add(wiki_data_embeddings[prompt_indices])\n",
    "\n",
    "        context = \"\"\n",
    "        \n",
    "        ## Get the top matches\n",
    "        ss, ii = prompt_index.search(question_embeddings, NUM_SENTENCES_INCLUDE)\n",
    "        for _s, _i in zip(ss[prompt_id], ii[prompt_id]):\n",
    "            context += processed_wiki_text_data.loc[prompt_indices]['text'].iloc[_i] + \" \"\n",
    "        \n",
    "    contexts.append(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "741afeb8-26de-4787-8b95-f598f723da5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmb_df['context'] = contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "9562ee74-29bf-4c5e-aae3-e3035af2357d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Among the European Union countries, which one has the largest land area?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'* First and second in number of hardcourt titles overall. * First and joint second in number of Major titles on grass courts. In most rivalries, the type of playing surface generally has a profound effect on the outcome; however, this rivalry was unusually even across different surfaces and conditions. Their 2019 final was historic as it was the longest final ever played at Wimbledon. '"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ii=3\n",
    "print(cmb_df.question[ii])\n",
    "cmb_df.context[ii]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4202fc5-7528-4eb6-a794-addbd8532579",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e99f368-2722-491a-9696-bff44275fe3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "066699a0-60ed-4eaa-a81c-86d181cc415d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForMultipleChoice were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DebertaV2ForMultipleChoice(\n",
       "  (deberta): DebertaV2Model(\n",
       "    (embeddings): DebertaV2Embeddings(\n",
       "      (word_embeddings): Embedding(128100, 1024, padding_idx=0)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "      (dropout): StableDropout()\n",
       "    )\n",
       "    (encoder): DebertaV2Encoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x DebertaV2Layer(\n",
       "          (attention): DebertaV2Attention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (query_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (pos_dropout): StableDropout()\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "            (output): DebertaV2SelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaV2Intermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DebertaV2Output(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (rel_embeddings): Embedding(512, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (pooler): ContextPooler(\n",
       "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (dropout): StableDropout()\n",
       "  )\n",
       "  (classifier): Linear(in_features=1024, out_features=1, bias=True)\n",
       "  (dropout): StableDropout()\n",
       ")"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"microsoft/deberta-v3-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForMultipleChoice.from_pretrained(model_name).cuda()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "d57bdf17-1233-49f4-bef5-a478afa83f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_207306/316385739.py:11: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  gg = full_df[~full_df.groundTruthAnswerEntity.isna()].groupby(\"question\").apply(combine_answers).reset_index()\n"
     ]
    }
   ],
   "source": [
    "def combine_answers(df):\n",
    "    l = df.answerEntity.tolist()\n",
    "    p = list(string.ascii_lowercase)[:len(l)]\n",
    "    zp = dict(zip(p, l))\n",
    "    zp = {k: [v] for k,v in zp.items()}\n",
    "    #print(np.where(df.correct.astype(int))[0])\n",
    "    zp['label'] = [np.where(df.correct.astype(int))[0][0]]\n",
    "    zp['prompt'] = [df.question.values[0]]\n",
    "    return pd.DataFrame(zp)\n",
    "\n",
    "gg = full_df[~full_df.groundTruthAnswerEntity.isna()].groupby(\"question\").apply(combine_answers).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "fbce581d-d308-4b09-b905-5d6d0c4b5292",
   "metadata": {},
   "outputs": [],
   "source": [
    "lts_cls = gg.columns[2:].drop(['label', 'prompt']).tolist()\n",
    "lts = ''.join(lts_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "299ad2f8-4633-49e4-bd4d-92a6e463916e",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = lts\n",
    "indices = list(range(len(lts)))\n",
    "\n",
    "option_to_index = {option: index for option, index in zip(options, indices)}\n",
    "index_to_option = {index: option for option, index in zip(options, indices)}\n",
    "\n",
    "def preprocess(example):\n",
    "    # The AutoModelForMultipleChoice class expects a set of question/answer pairs\n",
    "    # so we'll copy our question 5 times before tokenizing\n",
    "    first_sentence = [example['prompt']] * len(options)\n",
    "    second_sentence = []\n",
    "    for option in options:\n",
    "        second_sentence.append(example[option])\n",
    "    # Our tokenizer will turn our text into token IDs BERT can understand\n",
    "    tokenized_example = tokenizer(first_sentence, second_sentence, truncation=True)\n",
    "    tokenized_example['label'] = option_to_index[example['answer']]\n",
    "    return tokenized_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "7752ac18-db56-4173-9ab7-3e3a65c83c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorForMultipleChoice:\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    padding: Union[bool, str, PaddingStrategy] = True\n",
    "    max_length: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    \n",
    "    def __call__(self, features):\n",
    "        label_name = \"label\" if 'label' in features[0].keys() else 'labels'\n",
    "        labels = [feature.pop(label_name) for feature in features]\n",
    "        batch_size = len(features)\n",
    "        num_choices = len(features[0]['input_ids'])\n",
    "        flattened_features = [\n",
    "            [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n",
    "        ]\n",
    "        flattened_features = sum(flattened_features, [])\n",
    "        \n",
    "        batch = self.tokenizer.pad(\n",
    "            flattened_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n",
    "        batch['labels'] = torch.tensor(labels, dtype=torch.int64)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "0db35866-bb3e-4375-a09d-5956cdac768b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "preprocess() got an unexpected keyword argument 'remove_columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[215], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[1;32m      3\u001b[0m tokenized_test_dataset \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mfrom_pandas(\n\u001b[0;32m----> 4\u001b[0m     \u001b[43mgg\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprompt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlts_cls\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocess\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprompt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlts_cls\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m )\n\u001b[1;32m      6\u001b[0m tokenized_test_dataset \u001b[38;5;241m=\u001b[39m tokenized_test_dataset\u001b[38;5;241m.\u001b[39mremove_columns([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__index_level_0__\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      7\u001b[0m data_collator \u001b[38;5;241m=\u001b[39m DataCollatorForMultipleChoice(tokenizer\u001b[38;5;241m=\u001b[39mtokenizer)\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/pandas/core/frame.py:10468\u001b[0m, in \u001b[0;36mDataFrame.map\u001b[0;34m(self, func, na_action, **kwargs)\u001b[0m\n\u001b[1;32m  10465\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minfer\u001b[39m(x):\n\u001b[1;32m  10466\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\u001b[38;5;241m.\u001b[39m_map_values(func, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m> 10468\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43minfer\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmap\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/pandas/core/frame.py:10374\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[0;34m(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m  10360\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[1;32m  10362\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[1;32m  10363\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m  10364\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  10372\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m  10373\u001b[0m )\n\u001b[0;32m> 10374\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/pandas/core/apply.py:916\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw(engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine, engine_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_kwargs)\n\u001b[0;32m--> 916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/pandas/core/apply.py:1063\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1062\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1063\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1064\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1065\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_series_numba()\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/pandas/core/apply.py:1081\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1078\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1079\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[1;32m   1080\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[0;32m-> 1081\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1082\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[1;32m   1083\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/pandas/core/frame.py:10466\u001b[0m, in \u001b[0;36mDataFrame.map.<locals>.infer\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m  10465\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minfer\u001b[39m(x):\n\u001b[0;32m> 10466\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/unsloth_env/lib/python3.10/site-packages/pandas/core/algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1747\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: preprocess() got an unexpected keyword argument 'remove_columns'"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "tokenized_test_dataset = Dataset.from_pandas(\n",
    "    gg[['prompt', 'label'] + lts_cls].map(preprocess, remove_columns=['prompt', 'label'] + lts_cls)\n",
    ")\n",
    "tokenized_test_dataset = tokenized_test_dataset.remove_columns([\"__index_level_0__\"])\n",
    "data_collator = DataCollatorForMultipleChoice(tokenizer=tokenizer)\n",
    "test_dataloader = DataLoader(tokenized_test_dataset, batch_size=1, shuffle=False, collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd57991f-ab16-4d46-923a-d6b8b292421d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
